# -*- coding: utf-8 -*-
"""cookbook_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1apeM04QesVU8CR3C_M7FhQgtWq2RgH0N
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
# %matplotlib inline
np.set_printoptions(precision=3)
np.set_printoptions(threshold=999)

"""# Construct the sample network given in the paper"""

import random
import networkx as nx

# Initialize an empty graph
G = nx.Graph()

# Number of nodes to generate for each type
num_parties = 50
num_claims = 50

# Generate nodes for parties
for i in range(1, num_parties + 1):
    G.add_node(f"P{i}",
               claims=random.randint(1, 1000),
               fraudulent=random.randint(0, 10),
               non_fraudulent=random.randint(0, 10),
               bipartite=0)

# Generate nodes for claims
for i in range(1, num_claims + 1):
    G.add_node(f"C{i}",
               label=random.choice(["unknown", "no_fraud", "fraud"]),
               bipartite=1)

# Generate edges between party nodes and claim nodes
edges = []
for party_node in range(1, num_parties + 1):
    # Determine the number of edges for this party node
    num_edges = random.randint(1, min(10, num_claims))
    # Sample claims to connect to
    claim_nodes = random.sample(range(1, num_claims + 1), min(num_edges, 4))
    # Add edges to the list
    edges.extend([(f"P{party_node}", f"C{claim_node}", 1) for claim_node in claim_nodes])

# Make sure each node has at least one edge
for claim_node in range(1, num_claims + 1):
    if not G.degree[f"C{claim_node}"]:
        party_node = random.randint(1, num_parties)
        G.add_edge(f"P{party_node}", f"C{claim_node}", weight=1)  # Set weight to 1

# Add edges to the graph
G.add_weighted_edges_from(edges)

# Print the nodes and edges added
print("Nodes:")
print(G.nodes(data=True))
print("\nEdges:")
print(G.edges(data=True))

"""# Birank - Normalize Edgeweights
Birank normalizes the edge weights by the degree of the two adjacent nodes:
$$
Edge(u,v) = \frac{1}{\sqrt{deg(u)}\sqrt{deg(v)}}
$$

The effect is that the edges between nodes of high degrees, i.e. edge('P3, 'C1') = 0.2887 is lower than the nodes between nodes of lower degrees i.e. edge('P3', 'C3') = 0.356.  
During the calculation of the birank scores the updates will taken into account. So `Workshops`, `Claim Handler` and other `Involved Parties` with high degrees nodes's contribution to the birank score will be dampened.
"""

for u, v, d in G.edges(data=True):
    d['weight'] = d['weight'] / np.sqrt(G.degree[u] * G.degree[v])
    print( (u, G.degree[u], v, G.degree[v]), d)

"""$$
claimsbirank_j = \alpha \sum_{i=1}^{|P|} \frac{1}{\sqrt{deg(u)}\sqrt{deg(v)}}p_i + (1-\alpha)c^0_j,  
partiesbirank_i = \beta \sum_{j=1}^{|C|} \frac{1}{\sqrt{deg(u)}\sqrt{deg(v)}}c_j
$$

![image.png](attachment:60a67fc1-a47e-4f3f-be5b-aa12c08e3747.png)
"""

G.edges[('P50', 'C29')]

from networkx.algorithms import bipartite


print(bipartite.is_bipartite(G))
c = bipartite.color(G)
nodes = list(bipartite.color(G).keys())

color_map = {
    0: 'lightgreen',
    1: 'lightblue'
}
values = [color_map.get(value) for value in c.values()]
values

import networkx as nx
from networkx.algorithms import bipartite


plt.figure(figsize=(12,6))
pos = nx.spring_layout(G)
nx.draw_networkx_edges(G, pos, alpha=0.2)
nx.draw_networkx_nodes(G, pos, nodelist=nodes, node_color = values)
nx.draw_networkx_labels(G, pos)
plt.axis('off')
plt.show()

"""Export it as an Edgelist so that we can use calculate birank scores"""

edgelist_df_small_example = bipartite.edgelist.write_edgelist(G, 'edgelist_df_small_example', data=False)
edgelist_df_small_example

edgelist_df = pd.read_csv("edgelist_df_small_example", names=['Parties', 'Claims'], delim_whitespace=True)
edgelist_df

G.edges(data=True)

G.edges[('P4', 'C2')]
# G.edges[('P1', 'C2')]['weight']

def get_edgeweight(G, x):
    '''
    Returns the edge weight of a weighted graph

    Inputs:
        G::nx.Graph: NetworkX Graph with edge_weight attributes set to 'weight'
        x::pd.DataFrame: Edgelist DataFrame with first column the Source, second column the Dest

    Returns:
        pd.Series of the edgeweights between col1, col2 for every row in the DataFrame
    '''
    u, v = x[0], x[1]
    return G.edges[(u, v)]['weight']

edgelist_df['Weight'] = edgelist_df.apply(lambda x: get_edgeweight(G, x), axis=1)
edgelist_df

"""# 1. Birank Algorithm"""

import pandas as pd
import numpy as np
import scipy.sparse as spa


class Bipartite:
    """
    Class for handling bipartite networks using scipy's sparse matrix
    """
    def __init__(self):
        pass

    def set_edgelist(self, df, parties_col, claims_col, weight_col=None):
        """
        Method to set the edgelist.

        Input:
            df::pandas.DataFrame: the edgelist with at least two columns
            parties_col::string: column of the edgelist dataframe for Involved Parties nodes
            claims_col::string: column of the edgelist dataframe for Claims nodes
            weight_col::string: column of the edgelist dataframe for edge weights

        The edgelist should be represented by a dataframe.
        The dataframe needs at least two columns for the Involed Parties nodes and
        Claims nodes. An optional column can carry the edge weight.
        You need to specify the columns in the method parameters.
        """
        self.df = df
        self.parties_col = parties_col
        self.claims_col = claims_col
        self.weight_col = weight_col

        self._index_nodes()
        self._generate_adj()

    def generate_degree(self):
        """
        This method returns the degree of nodes in the bipartite network
        """
        parties_df = self.df.groupby(self.parties_col)[self.claims_col].nunique()
        parties_df = parties_df.to_frame(name='degree').reset_index()
        claims_df = self.df.groupby(self.claims_col)[self.parties_col].nunique()
        claims_df = claims_df.to_frame(name='degree').reset_index()
        return parties_df, claims_df

    def _index_nodes(self):
        """
        Representing the network with adjacency matrix requires indexing the parties
        and claims nodes
        """

        self.parties_ids = pd.DataFrame(
            self.df[self.parties_col].unique(),
            columns=[self.parties_col]
        ).reset_index()
        self.parties_ids = self.parties_ids.rename(columns={'index': 'parties_index'})

        self.claims_ids = pd.DataFrame(
            self.df[self.claims_col].unique(),
            columns=[self.claims_col]
        ).reset_index()
        self.claims_ids = self.claims_ids.rename(columns={'index': 'claims_index'})

        self.df = self.df.merge(self.parties_ids, on=self.parties_col)
        self.df = self.df.merge(self.claims_ids, on=self.claims_col)

    def _generate_adj(self):
        """
        Generating the adjacency matrix for the birparite network.
        The matrix has dimension: D * P where D is the number of top nodes
        and P is the number of bottom nodes
        """
        if self.weight_col is None:
            # set weight to 1 if weight column is not present
            weight = np.ones(len(self.df))
        else:
            weight = self.df[self.weight_col]
        self.W = spa.coo_matrix(
            (
                weight,
                (self.df['parties_index'].values, self.df['claims_index'].values)
            )
        )


    def generate_birank(self, normalizer='BiRank',
    alpha=0.85, beta=1, max_iter=500, tol=1.0e-5, prior=[], verbose=False):
        """
        Calculate the Fraud Score of bipartite networks directly.
        See paper https://arxiv.org/pdf/2009.08313.pdf
        for details.
        HITS, CoHITS, BGRM, BiRank Normalizer's implemented
        See paper https://ieeexplore.ieee.org/abstract/document/7572089/
        for details.

        Input:
            W::scipy's sparse matrix:Adjacency matrix of the bipartite network P*C
            normalizer::string:Choose which normalizer to use, see the paper for details
            alpha, beta::float:Damping factors for the rows and columns
            max_iter::int:Maximum iteration times. Set max_iter to 0 or 1 to return the prior_vector and prior
            tol::float:Error tolerance to check convergence
            prior::list: List of integers of known fradulent claims, e.g. Claim1, Claim3 Fraudulent -> [1,3]
            p: query vector for BiRank algorithm will then be arr, 1 at index prior-1, 0 otherwise, e.g. [1,0,1,0,0]
            verbose::boolean:If print iteration information

        Output:
             p, c::numpy.ndarray:BiRank scores for Parties and Claims
        """
        W = self.W
        df = self.df
        W = W.astype('float', copy=False)
        WT = W.T

        Kp = np.array(W.sum(axis=1)).flatten()
        Kc = np.array(W.sum(axis=0)).flatten()

        # avoid divided by zero issue
        Kp[np.where(Kp==0)] += 1
        Kc[np.where(Kc==0)] += 1

        # Normalizing the W weight matrix
        Kp_ = spa.diags(1/Kp)
        Kc_ = spa.diags(1/Kc)
        if normalizer == 'HITS':
            Sc = WT
            Sp = W
        elif normalizer == 'CoHITS':
            Sc = WT.dot(Kp_)
            Sp = W.dot(Kc_)
        elif normalizer == 'BGRM':
            Sc = Kc_.dot(WT).dot(Kp_)
            Sp = Sc.T
        elif normalizer == 'BiRank':
            Kp_bi = spa.diags(1/np.lib.scimath.sqrt(Kp))
            Kc_bi = spa.diags(1/np.lib.scimath.sqrt(Kc))
            Sc = Kc_bi.dot(WT).dot(Kp_bi)
            Sp = Sc.T
        # Sc and Sp are symmetric normalized weight matrix

        # Generate prior vector

        claims_col = self.claims_col
        claims_id = self.claims_ids.copy()
        claims_id['prior_from_prior'] = np.where(claims_id[claims_col].isin(prior), 1, 0)

        mask = claims_id[claims_col].isin(prior)
        prior_df_temp = claims_id[mask]

        prior_vector = np.zeros(len(claims_id))
        indices_to_put_prior = prior_df_temp['claims_index'].values
        np.put( prior_vector, indices_to_put_prior, np.ones(len(indices_to_put_prior)) )
        print( "No. of known prior fraudulent flags:", sum(prior_vector), "Length of prior_vector:", len(prior_vector) )

        claims_id['prior_from_setting_indices'] = prior_vector

        mask = claims_id['prior_from_setting_indices'] != claims_id['prior_from_prior']
        if claims_id[mask].shape[0] > 0:
            print("Prior vector not correctly set!")
            return prior_vector, prior

        parties_id = self.parties_ids.copy()
        # p: Parties (0 vector), c: Claims (prior)
        p0 = np.zeros(Kp_.shape[0])
        p_last = p0.copy()
        c0 = prior_vector
        c_last = c0.copy()

        if max_iter == 0 or max_iter == 1:
            print('Earning Stopping Warning: max_iter is {max_iter}'.format(max_iter=max_iter))
            c = c0
            p = p0
            return p, c

        for i in range(max_iter):
            c = alpha * (Sc.dot(p_last)) + (1-alpha) * c0
            p = beta * (Sp.dot(c_last)) + (1-beta) * p0

            if normalizer == 'HITS':
                c = c / c.sum()
                p = p / p.sum()

            err_c = np.absolute(c - c_last).sum()
            err_p = np.absolute(p - p_last).sum()
            if verbose:
                print(
                    "Iteration : {}; top error: {}; bottom error: {}".format(
                        i, err_p, err_c
                    )
                )
            if err_c < tol and err_p < tol:
                break
            p_last = p
            c_last = c
        parties_id['birank_score'] = p
        claims_id['birank_score'] = c
        return (parties_id[[self.parties_col, 'birank_score']],
                claims_id[[self.claims_col, 'birank_score']])

bn = Bipartite()
bn.set_edgelist(edgelist_df,  parties_col='Parties', claims_col='Claims')

prior = [node for node in G.nodes if G.nodes[node].get('label') == 'fraud']
parties_birank, claims_birank = bn.generate_birank(normalizer="BiRank", alpha=0.85, beta=1, prior=prior, max_iter=500, tol=1.0e-4, verbose=False)

claims_birank

parties_birank

"""![image.png](attachment:14a29271-aa30-4530-98fd-0fdf180f048d.png)

## 1.1 Checking the correctness of birank_scores

Notation:  
![image.png](attachment:bed5681b-d852-4730-9abe-3a05fdadee76.png)  
**p*** are the principle eigenvector of ($S^{T}S$) = `claims_birank`  
**u***are the principle eigenvector of ($SS^{T}$) = `parties_birank`
"""

p = claims_birank['birank_score'].values # p vector is the principal eigenvecctor of matrix S^T*S
u = parties_birank['birank_score'].values

W = bn.W.astype('float', copy=False)
WT = W.T

# Ku = scipy.array(W.sum(axis=1)).flatten()
# Kp = scipy.array(W.sum(axis=0)).flatten()

Ku = np.array(W.sum(axis=1)).flatten()
Kp = np.array(W.sum(axis=0)).flatten()


# avoid divided by zero issue
#Ku[np.where(Ku==0)] += 1
#Kp[np.where(Kp==0)] += 1

# Normalizing the W weight matrix
Ku_ = spa.diags(1/Ku)
Kp_ = spa.diags(1/Kp)

# BiRank Normalization
Ku_bi = spa.diags(1/np.sqrt(Ku))
Kp_bi = spa.diags(1/np.sqrt(Kp))
Sp = Kp_bi.dot(WT).dot(Ku_bi) # S^T
Su = Sp.T # S

ST_S = Sp.dot(Su)
S_ST = Su.dot(Sp)
print(ST_S.shape, S_ST.shape)

"""We define $$M: \alpha\beta[S^TS]$$ and by theorem 1: M's eigenvalues are bounded by $$[-\alpha\beta, \alpha\beta]$$

Recall:  
p is principal eigenvector of ST_S (Sc @ Sp),  
u is principal eigenvector of SS_T (Sp @ Sc)

$$
Ax = \lambda x
$$
is the form
$$
p \cdot S^{T}S = \lambda \cdot S^{T}S
$$
In our case since we specified $\alpha=0.85, \beta=1$, the bound is $[-0.85, 0.85]$
"""

eigenvalues, eigenvectors = np.linalg.eig(0.85 * S_ST.todense())
print(eigenvalues)
print(np.min(eigenvalues), np.max(eigenvalues))

eigenvalues, eigenvectors = np.linalg.eig(0.85 * ST_S.todense())
print(eigenvalues)
print(np.min(eigenvalues), np.max(eigenvalues))

"""This is equivalent to checking if the $S$ symmetric normalized matrix is a stochastic matrix, whose properties has maximum absolute eigenvalues 1, hence by definition M $\in[-\alpha\beta, \alpha\beta]$

![image.png](attachment:af444485-8aeb-4eb2-bebe-bfb4e0284ee4.png)
"""

prior_vector = np.zeros(num_claims)
for i in range(1, num_claims + 1):
    if G.nodes[f"C{i}"]['label'] == 'fraud':
        prior_vector[i - 1] = 1

print("Prior vector:", prior_vector)

prior_vector.shape

claims = np.linalg.inv( np.identity(ST_S.todense().shape[0]) - 0.85 * ST_S ) * ( 0.15 * prior_vector.reshape((50,1)) )
claims.flatten()

claims_birank['birank_score'].values

parties = np.linalg.inv( np.identity(S_ST.todense().shape[0]) - 0.85 * S_ST) * (0.15 * Su.todense() * prior_vector.reshape((50,1)))
parties.flatten()

parties_birank['birank_score'].values

"""## 1.2 A Note on Sparse Coordinate Matrix

$$
\text{claims_birank} = \alpha Wp + (1-\alpha) c^0,  
\text{parties_birank} = W^T c
$$
**W** is the symmetrically normalized weight matrix  
**c, p** are randomly initialized vector to hot start  
**c^0** is vector of know fraudulent claims (investigated and found to be fraudulent)  
$\alpha$ controls the trade-off between importance of network structure and query vector of known fraudulent claims

Higher $\alpha$ : More weightage placed on network structure, less weightage on prior information of known fraudulent claim  
Lower $\alpha$ : Less weightage placed on network structure, more weightage on prior information of known fraudulent claim
"""

from networkx.algorithms import bipartite
print(bipartite.is_bipartite(G))
# rows = ['P1', 'P2', 'P3', 'P4']
# cols = ['C1', 'C2', 'C3', 'C4', 'C5']
# W = bipartite.matrix.biadjacency_matrix(G, row_order = rows, column_order = cols)
# W.todense()

rows = [f"P{i}" for i in range(1, num_parties + 1)]
cols = [f"C{i}" for i in range(1, num_claims + 1)]

# Check if the graph is bipartite
print("Is the graph bipartite?", bipartite.is_bipartite(G))

# Calculate the biadjacency matrix
W = bipartite.biadjacency_matrix(G, row_order=rows, column_order=cols)
print("Biadjacency matrix:")
print(W.todense())

print(num_claims, num_parties)

"""Constructing the **W** unnormalized weight matrix to parse into the birank algorithm"""

bn.df['parties_index'].values # i

bn.df['claims_index'].values # j

n = len(bn.df['parties_index'].values)
weight = np.ones(n) # data
weight

"""![image.png](attachment:a668f0b9-c341-4a17-bda1-d552bd09ffa2.png)"""

import scipy.sparse as spa
W = spa.coo_matrix(
            (
                weight,
                (bn.df['parties_index'].values, bn.df['claims_index'].values)
            )
        )
W.todense()

"""Notice this is different from the adjacency matrix returned by NetworkX.  
This is because the columns in the W matrix is ['C2', 'C1', 'C3', 'C4', 'C5'], in the order of the `Claims` column in the given `edgelist_df`
"""

from networkx.algorithms import bipartite
import numpy as np

# Define the rows and columns
rows = [f"P{i}" for i in range(1, num_parties + 1)]
cols = [f"C{i}" for i in range(1, num_claims + 1)]

# Check if the graph is bipartite
print("Is the graph bipartite?", bipartite.is_bipartite(G))

# Calculate the biadjacency matrix
W = bipartite.biadjacency_matrix(G, row_order=rows, column_order=cols)
print("Biadjacency matrix:")
print(W.todense())

"""This is equal now

## 1.3 A Note on prior vector

![image.png](attachment:af1a6480-1cfc-4b17-9141-670728ba9e68.png)

In order words, the prior vector will be:
prior = [0, 0, 0, 1, 0]  
prior[3] == 1 because 'C4' is known to be fraudulent, the rest == 0
"""

prior = [node for node in G.nodes if G.nodes[node].get('label') == 'fraud']
# Generate prior vector
temp_df = pd.DataFrame(
    bn.df['Claims'].unique(),
    columns=['Claims']
).reset_index()

temp_df['prior'] = np.where(temp_df['Claims'].isin(prior), 1, 0)
temp_df

prior_vector = temp_df['prior'].values
prior_vector

"""## 1.4 A Note on why we use Birank instead of PageRank
`PageRank` scores nodes in a network to bring out nodes that are important in the network, we personalize `PageRank` to bring out the nodes that are important to a set of specific nodes, which in our perspective will be the known Fraduluent Claims. The resulting scores are then personalized towards these known fraudulent nodes, to reflect a preference for this nodes based on the homophilic nature of the network.

When calculating node centrality measures in bipartite networks, a common approach is to apply `PageRank` on the one-mode projection of the network. Basically for two sets $U$ and $P$, while there won't be any edges within nodes in $U$, but if two vertices in $U$ are connected via some node in $P$ as in our case, a projection onto $U$ will cause the information of the intermediate node in $P$ that causes $u_1$ and $u_2$ to be connected to be lost. You will lose the information on the intermediate node that connects $u_1$ and $u_2$ and distort the network topology.  

For better node ranking on bipartite networks wrt to our network structure, as is our use case with `Claims` and `Parties`, it is preferable to use a ranking algorithm that fully accounts for the topology of both modes of the network.
"""

bn = Bipartite()
# prior vector uses only known fradulent claims as query vector in BiRank
bn.set_edgelist(edgelist_df,  parties_col='Parties', claims_col='Claims')
# prior vector must be a list of the Claims ID known to be fraudulent
prior = [node for node in G.nodes if G.nodes[node].get('label') == 'fraud']
parties_birank, claims_birank = bn.generate_birank(normalizer="BiRank", alpha=0.85, beta=1, max_iter=200, tol=1.0e-4, prior=prior, verbose=False)
#β = 1, since only the network structure matters in the absence of the query vector.
claims_birank

# new_cols = {"Claims": "node", "Parties": "node"}
# df = parties_birank.rename(columns=new_cols).append(claims_birank.rename(columns=new_cols)).reset_index(drop=True)
# df

import pandas as pd

new_cols = {"Claims": "node", "Parties": "node"}
df = pd.concat([parties_birank.rename(columns=new_cols), claims_birank.rename(columns=new_cols)], ignore_index=True)
df

"""## Feature Engineering

![image.png](attachment:60a67fc1-a47e-4f3f-be5b-aa12c08e3747.png)
"""

def neighborhood(G, node, n):
    '''
    https://stackoverflow.com/questions/22742754/finding-the-n-degree-neighborhood-of-a-node
    Returns a list of nodes in the n-order neighborhood of
    '''
    path_lengths = nx.single_source_dijkstra_path_length(G, node, weight=None)
    return [node for node, length in path_lengths.items()
                    if length == n]

def generate_n1_q1(G, df, x):
    n1 = neighborhood(G, x, 1)
    mask = df['node'].isin(n1)
    temp_df = df[mask]
    n1_q1 = temp_df['birank_score'].quantile(0.25)
    return n1_q1

def generate_n1_q1(G, df, x):
    n1 = neighborhood(G, x, 1)
    mask = df['node'].isin(n1)
    temp_df = df[mask]
    n1_q1 = temp_df['birank_score'].quantile(0.25)
    return n1_q1

def generate_n1_med(G, df, x):
    n1 = neighborhood(G, x, 1)
    mask = df['node'].isin(n1)
    temp_df = df[mask]
    n1_med = temp_df['birank_score'].quantile(0.5)
    return n1_med

def generate_n1_max(G, df, x):
    n1 = neighborhood(G, x, 1)
    mask = df['node'].isin(n1)
    temp_df = df[mask]
    n1_max = temp_df['birank_score'].max()
    return n1_max

def generate_n2_q1(G, df, x):
    n2 = neighborhood(G, x, 2)
    mask = df['node'].isin(n2)
    temp_df = df[mask]
    n2_q1 = temp_df['birank_score'].quantile(0.25)
    return n2_q1

def generate_n2_med(G, df, x):
    n2 = neighborhood(G, x, 2)
    mask = df['node'].isin(n2)
    temp_df = df[mask]
    n2_med = temp_df['birank_score'].quantile(0.5)
    return n2_med

def generate_n2_max(G, df, x):
    n2 = neighborhood(G, x, 2)
    mask = df['node'].isin(n2)
    temp_df = df[mask]
    n2_max = temp_df['birank_score'].max()
    return n2_max

def generate_n1_size(G, df, x):
    '''
    Size of first order neighborhood is number of claims
    in which Involed Parties are engaged in
    Bipartite network: First Order Neighborhood of Claims
    is always an Involved Party
    '''
    n1 = neighborhood(G, x, 1)
    n1_size = len(n1)
    return n1_size

def generate_n2_size(G, df, x):
    '''
    Size of the second order neighborhood is number of Claims
    in which Involed Parties are engaged in
    Due to the nature of the Bipartite network, 2nd Order
    Neighborhood of Involved Parties is the number of Claims
    it is engaged in
    '''
    n2 = neighborhood(G, x, 2)
    n2_size = len(n2)
    return n2_size

df['n1_q1'] = df['node'].apply(lambda x: generate_n1_q1(G, df, x))
df['n1_med'] = df['node'].apply(lambda x: generate_n1_med(G, df, x))
df['n1_max'] = df['node'].apply(lambda x: generate_n1_max(G, df, x))
df['n2_q1'] = df['node'].apply(lambda x: generate_n2_q1(G, df, x))
df['n2_med'] = df['node'].apply(lambda x: generate_n2_med(G, df, x))
df['n2_max'] = df['node'].apply(lambda x: generate_n2_max(G, df, x))
df['n1_size'] = df['node'].apply(lambda x: generate_n1_size(G, df, x))
df['n2_size'] = df['node'].apply(lambda x: generate_n2_size(G, df, x))

df

def generate_n2_ratioFraud(G, x):
    if x not in G:
        return 0  # Return default value if node does not exist in the graph

    frauds_count = 0
    # Claims
    if x.startswith("C"):
        claims = neighborhood(G, x, 2)
        n2_size = len(claims)
        for claim in claims:
            if G.nodes[claim]['label'] == 'fraud':
                frauds_count += 1
        if frauds_count == 0:
            return 0
        else:
            n2_ratioFraud = frauds_count / n2_size
            return n2_ratioFraud

def generate_n2_ratioNonFraud(G, x):
    if x not in G:
        return 0  # Return default value if node does not exist in the graph

    no_frauds_count = 0
    # Claims
    if x.startswith("C"):
        claims = neighborhood(G, x, 2)
        n2_size = len(claims)
        for claim in claims:
            if G.nodes[claim]['label'] == 'no_fraud':
                no_frauds_count += 1
        if no_frauds_count == 0:
            return 0
        n2_ratioNonFraud = no_frauds_count / n2_size
        return n2_ratioNonFraud

def generate_n2_binFraud(G, x):
    if x not in G:
        return 0  # Return default value if node does not exist in the graph

    n2_binFraud = 0
    # Claim
    if x.startswith("C"):
        n2_binFraud = 0
        claims = neighborhood(G, x, 2)
        for claim in claims:
            if G.nodes[claim]['label'] == 'fraud':
                n2_binFraud = 1
                return n2_binFraud

    # Involved Parties
    subgraph_nodes = list(nx.single_source_shortest_path(G, x, 2).keys())
    subgraph = G.subgraph(subgraph_nodes)
    fraud_dict = nx.get_node_attributes(subgraph, 'fraudulent')
    n2 = neighborhood(subgraph, x, 2)
    n2_size = len(n2)

    for claim in n2:
        if claim not in fraud_dict:
            n2_binFraud = 1

    return n2_binFraud

"""![image.png](attachment:60a67fc1-a47e-4f3f-be5b-aa12c08e3747.png)"""

df['n2_ratioFraud'] = df['node'].apply(lambda x: generate_n2_ratioFraud(G, x))
df['n2_ratioNonFraud'] = df['node'].apply(lambda x: generate_n2_ratioNonFraud(G, x))
df['n2_binFraud'] = df['node'].apply(lambda x: generate_n2_binFraud(G, x))

df

unique_labels = df['node'].unique()
print("Unique node labels in DataFrame:", unique_labels)

for label in unique_labels:
    if label in G:
        print(f"Node {label} exists in the graph.")
    else:
        print(f"Node {label} does not exist in the graph.")

# Iterate over unique node labels in the DataFrame
for label in unique_labels:
    # Check if the node exists in the graph
    if label not in G:
        # Assign a default value of 0 to the node in your DataFrame
        # For example, if you have a column named 'n2_ratioFraud', you can assign 0 to it
        df.loc[df['node'] == label, 'n2_ratioFraud'] = 0
        df.loc[df['node'] == label, 'n2_ratioNonFraud'] = 0
        df.loc[df['node'] == label, 'n2_binFraud'] = 0

# Optionally, you can print a message to indicate that the assignment is done
print("Assigned 0 to unique nodes that do not exist in the graph.")

df['n2_ratioFraud'] = df['node'].apply(lambda x: generate_n2_ratioFraud(G, x))
df['n2_ratioNonFraud'] = df['node'].apply(lambda x: generate_n2_ratioNonFraud(G, x))
df['n2_binFraud'] = df['node'].apply(lambda x: generate_n2_binFraud(G, x))

df

claims_labels = {node: G.nodes[node]['label'] for node in G.nodes if node.startswith('C')}
claims_labels

claims_df = df[df['node'].str.startswith('C')].copy()

# Set target column based on label
# Add 'target' column based on labels
claims_df['target'] = [1 if claims_labels.get(node, '') in ['fraud', 'no_fraud'] else 0 for node in claims_df['node']]

# Display the resulting DataFrame
claims_df

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Define features (X) and target (y)
X = claims_df[['birank_score', 'n1_size', 'n2_size', 'n2_ratioFraud', 'n2_ratioNonFraud', 'n2_binFraud']]
y = claims_df['target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize logistic regression model
logreg = LogisticRegression()

# Train the model
logreg.fit(X_train, y_train)

# Predict on the testing set
y_pred = logreg.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.metrics import roc_auc_score, average_precision_score

# Calculate AUROC
auroc = roc_auc_score(y_test, logreg.predict_proba(X_test)[:, 1])
print("AUROC:", auroc)

# Calculate AUPR
aupr = average_precision_score(y_test, logreg.predict_proba(X_test)[:, 1])
print("AUPR:", aupr)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, confusion_matrix, roc_curve, precision_recall_curve
from sklearn.model_selection import learning_curve
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Define features (X) and target (y)
X = claims_df[['birank_score', 'n1_size', 'n2_size', 'n2_ratioFraud', 'n2_ratioNonFraud', 'n2_binFraud']]
y = claims_df['target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize logistic regression model
logreg = LogisticRegression()

# Train the model
logreg.fit(X_train, y_train)

# Predict on the testing set
y_pred = logreg.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Calculate AUROC
auroc = roc_auc_score(y_test, logreg.predict_proba(X_test)[:, 1])
print("AUROC:", auroc)

# Calculate AUPR
aupr = average_precision_score(y_test, logreg.predict_proba(X_test)[:, 1])
print("AUPR:", aupr)

# Plot learning curve
train_sizes, train_scores, valid_scores = learning_curve(logreg, X, y, train_sizes=[0.1, 0.3, 0.5, 0.7, 0.9], cv=5)
plt.figure()
plt.plot(train_sizes, train_scores.mean(axis=1), label='Training score')
plt.plot(train_sizes, valid_scores.mean(axis=1), label='Cross-validation score')
plt.xlabel('Training examples')
plt.ylabel('Score')
plt.title('Learning Curve')
plt.legend()
plt.show()

# Plot confusion matrix
conf_mat = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:, 1])
plt.plot(fpr, tpr)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()

# Plot precision-recall curve
precision, recall, _ = precision_recall_curve(y_test, logreg.predict_proba(X_test)[:, 1])
plt.plot(recall, precision)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

# Plot feature importance
feature_importance = logreg.coef_[0]
sorted_idx = np.argsort(feature_importance)
plt.barh(X.columns[sorted_idx], feature_importance[sorted_idx])
plt.xlabel('Feature Importance')
plt.title('Feature Importance Plot')
plt.show()

print(claims_df['target'].unique())

